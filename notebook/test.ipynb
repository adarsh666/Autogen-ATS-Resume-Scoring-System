{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e72a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69be50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e499abe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "from pydantic import BaseModel, Field, EmailStr, HttpUrl, field_validator\n",
    "\n",
    "\n",
    "class UserDetails(BaseModel):\n",
    "    name: str = Field(default=None, description=\"Name of the candidate\")\n",
    "    email: EmailStr = Field(default=None, description=\"Email address of the candidate\")\n",
    "    #linkedin_url: HttpUrl = Field(default=None, description=\"Linkdin url of the candidate\")\n",
    "    phone: str = Field(default=None, description=\"Valid contact number of the candidate with country code if avilable\")\n",
    "    skills: str = Field(default=None, description=\"Skills/technologies from the resume\")\n",
    "    work_related_skills: str = Field(default=None, description=\"Skills/technologies mentioned in work experience and project section, dont include sills from skill section of resume\")\n",
    "    current_company: str = Field(default=None, description=\"Extract recent company candidate worked or working from experience section\")\n",
    "    current_location: str = Field(default=None, description=\"Extract recent work location of candidate worked or working from experience section\")\n",
    "    last_company_experience_in_months: int = Field(default=None, description=f\"Calculate candidate's total months of working experience from date of joining in the most recent company date till today's date {datetime.date.today().strftime('%m/%d/%Y')}\\\n",
    "                                                                             if candidate is not working in any company then calculate from date of joining in the most recent company to last date he had worked. If there are career gpas then do not consider that period \")\n",
    "    total_work_experience_in_months: int = Field(default=None, description=f\"Calculate candidate's total months of working experience from start date of career date till today's date {datetime.date.today().strftime('%m/%d/%Y')}. \\\n",
    "                                                                             if candidate in not working in any company then calculate from start date of career to last date he had worked. If there are career gpas then do not consider that period\" )\n",
    "    education: str = Field(default=None, description=\"Heighest education of the candidate, if there are multiple degrees then extract the highest degree\")\n",
    "\n",
    "    # @field_validator('linkedin_url')\n",
    "    # def validate_linkedin_url(cls, v):\n",
    "    #     if v is None:\n",
    "    #         return v  # Optional field\n",
    "    #     pattern = r'^https:\\/\\/(www\\.)?linkedin\\.com\\/in\\/[A-Za-z0-9-_%]+\\/?$'\n",
    "    #     if not re.match(pattern, v):\n",
    "    #         raise ValueError('Invalid LinkedIn profile URL. Must be like \"https://www.linkedin.com/in/username/\"')\n",
    "    #    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "794e97c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Extract relevent  information:\n",
    "Adarsh Reghuvaran Machine Learning Engineer\n",
    "+91-9400831199\n",
    "Linkedin\n",
    "adarsh.reghuvaran@gmail.com\n",
    "SKILLS\n",
    "• Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI,\n",
    "Prompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure,\n",
    "Data Analysis\n",
    "EXPERIENCE\n",
    "Jan 2025 - Present\n",
    "Data Scientist\n",
    "Saraca Solutions\n",
    "Designed and implemented a multi-agent conversational AI system using LangGraph to coordinate specialized agents for complex\n",
    "question answering. The system uses RAG to pull contextually relevant information from multiple document sources.\n",
    "Designed and developed an AI-powered chatbot using LangChain and RAG (Retrieval-Augmented Generation) to handle\n",
    "employee queries efficiently.\n",
    "Dec 2021 - Jan 2025\n",
    "Machine Learning Engineer\n",
    "Trenser Technology Solutions (P) Ltd.\n",
    "Built medical companion chat-bot which gives medical advices to user based on user's medical history as well as information from\n",
    "medical documents using Retrieval-Augmented Generation (RAG). Chat-bot was build using openai's GPT-4.\n",
    "Worked in the development of Surgical assistance tool which inclues implementing research paper techniques to improve the\n",
    "existing deep learning model based on FPN architecture such as implemented ST++(https://arxiv.org/abs/2106.05095), making neck\n",
    "of the model lighter to reduce inference time, reduced the time in image processing module from 300ms to 3ms by trying out\n",
    "different image processing techniques using OpenCV and Numpy\n",
    "Improved existing object detection model used in mobile application by trying out data augmentation, increased the size of minority\n",
    "class images using data augmentatation and changing entire model which had improved to accuracy on test set from 94% to 97%.\n",
    "Assistant Data Scientist\n",
    "Curvelogics Advanced Technology Solutions Pvt Ltd\n",
    "Sep 2020 - Dec 2021\n",
    "Designed and implemented NLP-based system to classify documents using Machine learning Naive Bayes classifier and extract\n",
    "information from documents using NLTK, Scipy and Regular expression\n",
    "Built tool for analyzing people’s sentiment towards a topic by analyzing recent tweets using Large Language models such as\n",
    "BERT, T5 etc using huggingface library and Tensorflow. Responsiblities include collecting training data, training model and\n",
    "evaluation.\n",
    "Build Segmentation model to identify dents/cracks in vehicles for an insurance company using Detectron model. Responsiblities\n",
    "include labeling training data and model training.\n",
    "Build tracking mechanism in CCTV live feeds to track people and send notification when detected.\n",
    "Build Classification model using Light Gradient Boost Classifier to predict if the client will subscribe an isurance using direct\n",
    "marketing campaign for a banking institution using heavily unbalanced dataset which contains only 8% positive class. Model was\n",
    "able to achieve recall of 0.77 on test set.Responsiblities include feature engineering, outlier removal and model building.\n",
    "PROJECTS\n",
    "NLP: Named Entity Recognition\n",
    "https://github.com/adarsh666/NER/blob/main/Twitter_NER.ipynb\n",
    "Utilized Named Entity Recognition (NER) to automatically identify and extract important content from tweets, contributing to\n",
    "enhanced understanding of user-generated content and trends.\n",
    "Model used was bert-base-uncased using huggingface library. Model was able to achieve accuracy of 97.11% in traing set and\n",
    "96.21% on test set.\n",
    "Recommender System\n",
    "https://github.com/adarsh666/RecommenderSystem/blob/main/Recommender_Systems.ipynb\n",
    "Created Recommender Systems using Item - based, User - based and Matrix Factorization to show personalized movie\n",
    "recommendations based on ratings given by a user and other users similar to them in order to improve user experience with\n",
    "RMSE score of close to 0.8\n",
    "EDUCATION\n",
    "Woolf\n",
    "MS in Computer Science 90%2024\n",
    "Kerala University\n",
    "BE/B.Tech/BS in Mechanical Engineering2015\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f37b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_model_client = OpenAIChatCompletionClient(\n",
    "    model='gpt-4o-mini',\n",
    "    api_key=api_key,\n",
    "    response_format=UserDetails\n",
    ")\n",
    "\n",
    "\n",
    "my_assistant_2 = AssistantAgent(name='Assistant2',model_client=structured_model_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4be13162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[TextMessage(id='eacb6623-f0c8-4a73-bb84-3289d4a40c8b', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 10, 6, 26, 22, 390332, tzinfo=datetime.timezone.utc), content=\"\\nExtract relevent  information:\\nAdarsh Reghuvaran Machine Learning Engineer\\n+91-9400831199\\nLinkedin\\nadarsh.reghuvaran@gmail.com\\nSKILLS\\n• Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI,\\nPrompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure,\\nData Analysis\\nEXPERIENCE\\nJan 2025 - Present\\nData Scientist\\nSaraca Solutions\\nDesigned and implemented a multi-agent conversational AI system using LangGraph to coordinate specialized agents for complex\\nquestion answering. The system uses RAG to pull contextually relevant information from multiple document sources.\\nDesigned and developed an AI-powered chatbot using LangChain and RAG (Retrieval-Augmented Generation) to handle\\nemployee queries efficiently.\\nDec 2021 - Jan 2025\\nMachine Learning Engineer\\nTrenser Technology Solutions (P) Ltd.\\nBuilt medical companion chat-bot which gives medical advices to user based on user's medical history as well as information from\\nmedical documents using Retrieval-Augmented Generation (RAG). Chat-bot was build using openai's GPT-4.\\nWorked in the development of Surgical assistance tool which inclues implementing research paper techniques to improve the\\nexisting deep learning model based on FPN architecture such as implemented ST++(https://arxiv.org/abs/2106.05095), making neck\\nof the model lighter to reduce inference time, reduced the time in image processing module from 300ms to 3ms by trying out\\ndifferent image processing techniques using OpenCV and Numpy\\nImproved existing object detection model used in mobile application by trying out data augmentation, increased the size of minority\\nclass images using data augmentatation and changing entire model which had improved to accuracy on test set from 94% to 97%.\\nAssistant Data Scientist\\nCurvelogics Advanced Technology Solutions Pvt Ltd\\nSep 2020 - Dec 2021\\nDesigned and implemented NLP-based system to classify documents using Machine learning Naive Bayes classifier and extract\\ninformation from documents using NLTK, Scipy and Regular expression\\nBuilt tool for analyzing people’s sentiment towards a topic by analyzing recent tweets using Large Language models such as\\nBERT, T5 etc using huggingface library and Tensorflow. Responsiblities include collecting training data, training model and\\nevaluation.\\nBuild Segmentation model to identify dents/cracks in vehicles for an insurance company using Detectron model. Responsiblities\\ninclude labeling training data and model training.\\nBuild tracking mechanism in CCTV live feeds to track people and send notification when detected.\\nBuild Classification model using Light Gradient Boost Classifier to predict if the client will subscribe an isurance using direct\\nmarketing campaign for a banking institution using heavily unbalanced dataset which contains only 8% positive class. Model was\\nable to achieve recall of 0.77 on test set.Responsiblities include feature engineering, outlier removal and model building.\\nPROJECTS\\nNLP: Named Entity Recognition\\nhttps://github.com/adarsh666/NER/blob/main/Twitter_NER.ipynb\\nUtilized Named Entity Recognition (NER) to automatically identify and extract important content from tweets, contributing to\\nenhanced understanding of user-generated content and trends.\\nModel used was bert-base-uncased using huggingface library. Model was able to achieve accuracy of 97.11% in traing set and\\n96.21% on test set.\\nRecommender System\\nhttps://github.com/adarsh666/RecommenderSystem/blob/main/Recommender_Systems.ipynb\\nCreated Recommender Systems using Item - based, User - based and Matrix Factorization to show personalized movie\\nrecommendations based on ratings given by a user and other users similar to them in order to improve user experience with\\nRMSE score of close to 0.8\\nEDUCATION\\nWoolf\\nMS in Computer Science 90%2024\\nKerala University\\nBE/B.Tech/BS in Mechanical Engineering2015\\n\", type='TextMessage'), TextMessage(id='7d0c5d0d-a18c-4af3-b91d-0ce598973622', source='Assistant2', models_usage=RequestUsage(prompt_tokens=1296, completion_tokens=201), metadata={}, created_at=datetime.datetime(2025, 8, 10, 6, 26, 26, 918917, tzinfo=datetime.timezone.utc), content='{\"name\":\"Adarsh Reghuvaran\",\"email\":\"adarsh.reghuvaran@gmail.com\",\"phone\":\"+91-9400831199\",\"skills\":\"Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI, Prompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure, Data Analysis\",\"work_related_skills\":\"LangChain, RAG (Retrieval-Augmented Generation), openai\\'s GPT-4, FPN architecture, OpenCV, Naive Bayes classifier, NLTK, Scipy, huggingface library, Tensorflow, Detectron model, Light Gradient Boost Classifier\",\"current_company\":\"Saraca Solutions\",\"current_location\":\"\",\"last_company_experience_in_months\":10,\"total_work_experience_in_months\":61,\"education\":\"MS in Computer Science\"}', type='TextMessage')] stop_reason=None\n"
     ]
    }
   ],
   "source": [
    "result = await my_assistant_2.run(task=text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5af446fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76424214",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = result.messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09b77078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Adarsh Reghuvaran',\n",
       " 'email': 'adarsh.reghuvaran@gmail.com',\n",
       " 'phone': '+91-9400831199',\n",
       " 'skills': 'Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI, Prompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure, Data Analysis',\n",
       " 'work_related_skills': \"LangChain, RAG (Retrieval-Augmented Generation), openai's GPT-4, FPN architecture, OpenCV, Naive Bayes classifier, NLTK, Scipy, huggingface library, Tensorflow, Detectron model, Light Gradient Boost Classifier\",\n",
       " 'current_company': 'Saraca Solutions',\n",
       " 'current_location': '',\n",
       " 'last_company_experience_in_months': 10,\n",
       " 'total_work_experience_in_months': 61,\n",
       " 'education': 'MS in Computer Science'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = json.loads(res)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ace6886d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82af06ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Adarsh Reghuvaran',\n",
       " 'email': 'adarsh.reghuvaran@gmail.com',\n",
       " 'phone': '+91-9400831199',\n",
       " 'skills': 'Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI, Prompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure, Data Analysis',\n",
       " 'work_related_skills': \"LangChain, RAG (Retrieval-Augmented Generation), openai's GPT-4, FPN architecture, OpenCV, Naive Bayes classifier, NLTK, Scipy, huggingface library, Tensorflow, Detectron model, Light Gradient Boost Classifier\",\n",
       " 'current_company': 'Saraca Solutions',\n",
       " 'current_location': '',\n",
       " 'last_company_experience_in_months': 10,\n",
       " 'total_work_experience_in_months': 61,\n",
       " 'education': 'MS in Computer Science'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"name\":\"Adarsh Reghuvaran\",\"email\":\"adarsh.reghuvaran@gmail.com\",\"phone\":\"+91-9400831199\",\"skills\":\"Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI, Prompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure, Data Analysis\",\"work_related_skills\":\"LangChain, RAG (Retrieval-Augmented Generation), openai's GPT-4, FPN architecture, OpenCV, Naive Bayes classifier, NLTK, Scipy, huggingface library, Tensorflow, Detectron model, Light Gradient Boost Classifier\",\"current_company\":\"Saraca Solutions\",\"current_location\":\"\",\"last_company_experience_in_months\":10,\"total_work_experience_in_months\":61,\"education\":\"MS in Computer Science\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54532349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from autogen_core.memory import Memory, MemoryContent, MemoryMimeType\n",
    "\n",
    "\n",
    "class SimpleDocumentIndexer:\n",
    "    \"\"\"Basic document indexer for AutoGen Memory.\"\"\"\n",
    "\n",
    "    def __init__(self, memory: Memory, chunk_size: int = 2500) -> None:\n",
    "        self.memory = memory\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    async def _fetch_content(self, source: str) -> str:\n",
    "        \"\"\"Fetch content from URL or file.\"\"\"\n",
    "        if source.startswith((\"http://\", \"https://\")):\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(source) as response:\n",
    "                    return await response.text()\n",
    "        else:\n",
    "            async with aiofiles.open(source, \"r\", encoding=\"utf-8\") as f:\n",
    "                return await f.read()\n",
    "\n",
    "    def _strip_html(self, text: str) -> str:\n",
    "        \"\"\"Remove HTML tags and normalize whitespace.\"\"\"\n",
    "        text = re.sub(r\"<[^>]*>\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into fixed-size chunks.\"\"\"\n",
    "        chunks: list[str] = []\n",
    "        # Just split text into fixed-size chunks\n",
    "        for i in range(0, len(text), self.chunk_size):\n",
    "            chunk = text[i : i + self.chunk_size]\n",
    "            chunks.append(chunk.strip())\n",
    "        return chunks\n",
    "\n",
    "    async def index_documents(self, sources: List[str]) -> int:\n",
    "        \"\"\"Index documents into memory.\"\"\"\n",
    "        total_chunks = 0\n",
    "\n",
    "        for source in sources:\n",
    "            try:\n",
    "                content = await self._fetch_content(source)\n",
    "\n",
    "                # Strip HTML if content appears to be HTML\n",
    "                if \"<\" in content and \">\" in content:\n",
    "                    content = self._strip_html(content)\n",
    "\n",
    "                chunks = self._split_text(content)\n",
    "\n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    await self.memory.add(\n",
    "                        MemoryContent(\n",
    "                            content=chunk, mime_type=MemoryMimeType.TEXT, metadata={\"source\": source, \"chunk_index\": i}\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                total_chunks += len(chunks)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error indexing {source}: {str(e)}\")\n",
    "\n",
    "        return total_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74c52e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:52<00:00, 1.58MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 5 chunks from 1 AutoGen documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.memory.chromadb import ChromaDBVectorMemory, PersistentChromaDBVectorMemoryConfig\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "\n",
    "# Initialize vector memory\n",
    "\n",
    "rag_memory = ChromaDBVectorMemory(\n",
    "    config=PersistentChromaDBVectorMemoryConfig(\n",
    "        collection_name=\"autogen_docs\",\n",
    "        persistence_path=os.path.join(str(Path.home()), \".chromadb_autogen\"),\n",
    "        k=3,  # Return top 3 results\n",
    "        score_threshold=0.4,  # Minimum similarity score\n",
    "    )\n",
    ")\n",
    "\n",
    "await rag_memory.clear()  # Clear existing memory\n",
    "\n",
    "\n",
    "# Index AutoGen documentation\n",
    "async def index_autogen_docs() -> None:\n",
    "    indexer = SimpleDocumentIndexer(memory=rag_memory)\n",
    "    sources = [\n",
    "        \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "    ]\n",
    "    chunks: int = await indexer.index_documents(sources)\n",
    "    print(f\"Indexed {chunks} chunks from {len(sources)} AutoGen documents\")\n",
    "\n",
    "\n",
    "await index_autogen_docs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e77bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc13acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b40233c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adarsh/Autogen-ATS-Resume-Scoring-System/notebook'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decb0615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/adarsh/Autogen-ATS-Resume-Scoring-System'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_dir = os.path.dirname(current_dir)\n",
    "parent_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a32dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317cce16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from database.vector_store import VectorDB\n",
    "\n",
    "vector_db = VectorDB()\n",
    "collection = vector_db.get_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ffd60e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1bbc032",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcollection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpeek\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Autogen-ATS-Resume-Scoring-System/.venv/lib/python3.12/site-packages/chromadb/api/models/Collection.py:156\u001b[39m, in \u001b[36mCollection.peek\u001b[39m\u001b[34m(self, limit)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpeek\u001b[39m(\u001b[38;5;28mself\u001b[39m, limit: \u001b[38;5;28mint\u001b[39m = \u001b[32m10\u001b[39m) -> GetResult:\n\u001b[32m    147\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get the first few results in the database up to limit\u001b[39;00m\n\u001b[32m    148\u001b[39m \n\u001b[32m    149\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \u001b[33;03m        GetResult: A GetResult object containing the results.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_peek_response(\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_peek\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Autogen-ATS-Resume-Scoring-System/.venv/lib/python3.12/site-packages/chromadb/api/rust.py:330\u001b[39m, in \u001b[36mRustBindingsAPI._peek\u001b[39m\u001b[34m(self, collection_id, n, tenant, database)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_peek\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    328\u001b[39m     database: \u001b[38;5;28mstr\u001b[39m = DEFAULT_DATABASE,\n\u001b[32m    329\u001b[39m ) -> GetResult:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIncludeMetadataDocumentsEmbeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Autogen-ATS-Resume-Scoring-System/.venv/lib/python3.12/site-packages/chromadb/api/rust.py:363\u001b[39m, in \u001b[36mRustBindingsAPI._get\u001b[39m\u001b[34m(self, collection_id, ids, where, limit, offset, where_document, include, tenant, database)\u001b[39m\n\u001b[32m    351\u001b[39m ids_amount = \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    353\u001b[39m     CollectionGetEvent(\n\u001b[32m    354\u001b[39m         collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    360\u001b[39m     )\n\u001b[32m    361\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m rust_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m GetResult(\n\u001b[32m    376\u001b[39m     ids=rust_response.ids,\n\u001b[32m    377\u001b[39m     embeddings=rust_response.embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     metadatas=rust_response.metadatas,\n\u001b[32m    383\u001b[39m )\n",
      "\u001b[31mInternalError\u001b[39m: Error executing plan: Internal error: Error creating hnsw segment reader: Nothing found on disk"
     ]
    }
   ],
   "source": [
    "collection.peek()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7707e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import get_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "491d7372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bfac555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import uuid\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "# # Example usage\n",
    "# pdf_file = \"/home/adarsh/agentic/resumes_new/adarsh_saraca.pdf\"\n",
    "# content = extract_text_from_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9367eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"/home/adarsh/agentic/ai jd2.pdf\"\n",
    "content = extract_text_from_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e74ed6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job Description   \\nRole:  AI/ML Engineer for Telecom Analytics  \\nDesignation:  ITA & Above,        JOB LOCATION : Hyderabad/Chennai,    ROLE TENURE :  Long Term    \\nOur Team  \\nThe team is looking for AI/ML Developers with solid background in Machine learning and Deep \\nLearning. The developer will be expected to work as an individual contributor with the guidance from \\ndomain experts and should have innovative thinking ability. The engineer should have experience \\nworking on large sc ale Machine learning/deep learning projects with large scale of data preferably in \\nTelecom domain. He/she should have sound understanding of feature engineering, model selection, \\ntraining, validation and test metrics.  \\nRESPONSIBILITIES:  \\n● Develop algorithms a nd early learning software for customers.  \\n● Create documentation and training materials.  \\n● Build, Train and Test multiple machine learning models.  \\n● Hyperparameter tuning of trained models to achieve best possible results.  \\n● Individual Contributor  \\n● Interact with a cross -functional team of data scientists, engineers, and domain experts to \\ndetermine requirements and functionality.  \\n● Work with on -shore and off -shore engineering teams to support your software development \\nefforts.  \\nTECHNICAL COMPETENCIES & EXPERIENCE:  \\n● Overall 5+ years of experience.  \\n● 3+ years of experience in  AI/ ML/ NLP.  \\n● Experience in Telecom domain is good to have.  \\n● Strong experience of Deep Learning frameworks – TensorFlow, Keras, PyTorch.  \\n● Strong programming skills in python and experience of Ski -Learn/NumPy libraries. (C++ is \\nAdd on)  \\n● Experience in data mining, Text Mining, working and creating data architectures  \\n● Experience in cloud services e.g. PaaS and SaaS, Rest API, serverless functions  \\n● Understanding of Compute Engines, VM, C ontainers is nice to have  ● Problem -solving aptitude  \\n● Understanding and experience with leading supervised and unsupervised machine learning \\nmethods such as Regression, Neural Networks, Deep Learning, KNN, Naive Bayes, SVM, \\nDecision Trees, Random Forest, Gradient Boosting, Ensemble methods, text mining.  \\n● Experience in Kaggle/MICCA or any online data science competitions  \\n● IT Skills: MS office tools (Word, Excel, PowerPoint)  \\nEDUCATION:  \\nB.Tech./ B.E./ M.E./ M.Tech./M.S. in Computer Science, Information Technology, ECE, EE, EEE,  \\nLANGUAGE & SKILLS:  \\n● Proficient in English language  \\n● Good written and Verbal communication skill  \\nWrite to Ramesh.Kumar@zureetelecom.com  \\n '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "472f239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5402fe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_tools.tools import add_document_to_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc83a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job Description added to vector store successfully.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_document_to_vectorstore(content, id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b979e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d8ddd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Agents.jd_analysis_agent import get_jd_analysis_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0361db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = get_jd_analysis_agent()\n",
    "result = await agent.run(task=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e486712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextMessage(id='23557e22-41d2-4d85-8ec4-6da557cc0996', source='user', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 12, 18, 6, 42, 616489, tzinfo=datetime.timezone.utc), content=\"Adarsh ReghuvaranMachine Learning Engineer\\n+91-9400831199 Linkedin adarsh.reghuvaran@gmail.com\\nSKILLS\\n•Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI, \\nPrompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure, \\nData Analysis\\nEXPERIENCE\\nData Scientist Jan 2025 - Present\\nSaraca Solutions\\nDesigned and implemented a multi-agent conversational AI system using LangGraph to coordinate specialized agents for complex\\nquestion answering. The system uses RAG to pull contextually relevant information from multiple document sources.\\nDesigned and developed an AI-powered chatbot using LangChain and RAG (Retrieval-Augmented Generation) to handle\\nemployee queries efficiently.\\nMachine Learning Engineer Dec 2021 - Jan 2025\\nTrenser Technology Solutions (P) Ltd.\\nBuilt medical companion chat-bot which gives medical advices to user based on user's medical history as well as information from\\nmedical documents using Retrieval-Augmented Generation (RAG). Chat-bot was build using openai's GPT-4.\\nWorked in the development of Surgical assistance tool which inclues implementing research paper techniques to improve the\\nexisting deep learning model based on FPN architecture such as implemented ST++(https://arxiv.org/abs/2106.05095), making neck\\nof the model lighter to reduce inference time, reduced the time in image processing module from 300ms to 3ms by trying out\\ndifferent image processing techniques using OpenCV and Numpy\\nImproved existing object detection model used in mobile application by trying out data augmentation, increased the size of minority\\nclass images using data augmentatation and changing entire model which had improved to accuracy on test set from 94% to 97%.\\nAssistant Data Scientist Sep 2020 - Dec 2021\\nCurvelogics Advanced Technology Solutions Pvt Ltd\\nDesigned and implemented NLP-based system to classify documents using Machine learning Naive Bayes classifier and extract\\ninformation from documents using NLTK, Scipy and Regular expression\\nBuilt tool for analyzing people’s sentiment towards a topic by analyzing recent tweets using Large Language models such as\\nBERT, T5 etc using huggingface library and Tensorflow. Responsiblities include collecting training data, training model and\\nevaluation.\\nBuild Segmentation model to identify dents/cracks in vehicles for an insurance company using Detectron model. Responsiblities\\ninclude labeling training data and model training.\\nBuild tracking mechanism in CCTV live feeds to track people and send notification when detected.\\nBuild Classification model using Light Gradient Boost Classifier to predict if the client will subscribe an isurance using direct\\nmarketing campaign for a banking institution using heavily unbalanced dataset which contains only 8% positive class. Model was\\nable to achieve recall of 0.77 on test set.Responsiblities include feature engineering, outlier removal and model building.\\nPROJECTS\\nNLP: Named Entity Recognition\\nhttps://github.com/adarsh666/NER/blob/main/Twitter_NER.ipynb\\nUtilized Named Entity Recognition (NER) to automatically identify and extract important content from tweets, contributing to\\nenhanced understanding of user-generated content and trends.\\nModel used was bert-base-uncased using huggingface library. Model was able to achieve accuracy of 97.11% in traing set and\\n96.21% on test set.\\nRecommender System\\nhttps://github.com/adarsh666/RecommenderSystem/blob/main/Recommender_Systems.ipynb\\nCreated Recommender Systems using Item - based, User - based and Matrix Factorization to show personalized movie\\nrecommendations based on ratings given by a user and other users similar to them in order to improve user experience with\\nRMSE score of close to 0.8\\nEDUCATION\\nWoolf 2024\\nMS in Computer Science90%\\nKerala University 2015\\nBE/B.Tech/BS in Mechanical Engineering\", type='TextMessage'),\n",
       " ToolCallRequestEvent(id='dd8d8d17-c8b3-49c5-92cb-d818066a8b4e', source='job_description_analysis_agent', models_usage=RequestUsage(prompt_tokens=958, completion_tokens=874), metadata={}, created_at=datetime.datetime(2025, 8, 12, 18, 7, 6, 19512, tzinfo=datetime.timezone.utc), content=[FunctionCall(id='call_PUjgeXvl3E38XuFdApFMMVZJ', arguments='{\"resume_data\":\"Adarsh ReghuvaranMachine Learning Engineer\\\\n+91-9400831199 Linkedin adarsh.reghuvaran@gmail.com\\\\nSKILLS\\\\n•Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI,\\\\nPrompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure,\\\\nData Analysis\\\\nEXPERIENCE\\\\nData Scientist Jan 2025 - Present\\\\nSaraca Solutions\\\\nDesigned and implemented a multi-agent conversational AI system using LangGraph to coordinate specialized agents for complex\\\\nquestion answering. The system uses RAG to pull contextually relevant information from multiple document sources.\\\\nDesigned and developed an AI-powered chatbot using LangChain and RAG (Retrieval-Augmented Generation) to handle\\\\nemployee queries efficiently.\\\\nMachine Learning Engineer Dec 2021 - Jan 2025\\\\nTrenser Technology Solutions (P) Ltd.\\\\nBuilt medical companion chat-bot which gives medical advices to user based on user\\'s medical history as well as information from\\\\nmedical documents using Retrieval-Augmented Generation (RAG). Chat-bot was build using openai\\'s GPT-4.\\\\nWorked in the development of Surgical assistance tool which inclues implementing research paper techniques to improve the\\\\nexisting deep learning model based on FPN architecture such as implemented ST++(https://arxiv.org/abs/2106.05095), making neck\\\\nof the model lighter to reduce inference time, reduced the time in image processing module from 300ms to 3ms by trying out\\\\ndifferent image processing techniques using OpenCV and Numpy\\\\nImproved existing object detection model used in mobile application by trying out data augmentation, increased the size of minority\\\\nclass images using data augmentatation and changing entire model which had improved to accuracy on test set from 94% to 97%.\\\\nAssistant Data Scientist Sep 2020 - Dec 2021\\\\nCurvelogics Advanced Technology Solutions Pvt Ltd\\\\nDesigned and implemented NLP-based system to classify documents using Machine learning Naive Bayes classifier and extract\\\\ninformation from documents using NLTK, Scipy and Regular expression\\\\nBuilt tool for analyzing people’s sentiment towards a topic by analyzing recent tweets using Large Language models such as\\\\nBERT, T5 etc using huggingface library and Tensorflow. Responsiblities include collecting training data, training model and\\\\nevaluation.\\\\nBuild Segmentation model to identify dents/cracks in vehicles for an insurance company using Detectron model. Responsiblities\\\\ninclude labeling training data and model training.\\\\nBuild tracking mechanism in CCTV live feeds to track people and send notification when detected.\\\\nBuild Classification model using Light Gradient Boost Classifier to predict if the client will subscribe an isurance using direct\\\\nmarketing campaign for a banking institution using heavily unbalanced dataset which contains only 8% positive class. Model was\\\\nable to achieve recall of 0.77 on test set.Responsiblities include feature engineering, outlier removal and model building.\\\\nPROJECTS\\\\nNLP: Named Entity Recognition\\\\nhttps://github.com/adarsh666/NER/blob/main/Twitter_NER.ipynb\\\\nUtilized Named Entity Recognition (NER) to automatically identify and extract important content from tweets, contributing to\\\\nenhanced understanding of user-generated content and trends.\\\\nModel used was bert-base-uncased using huggingface library. Model was able to achieve accuracy of 97.11% in traing set and\\\\n96.21% on test set.\\\\nRecommender System\\\\nhttps://github.com/adarsh666/RecommenderSystem/blob/main/Recommender_Systems.ipynb\\\\nCreated Recommender Systems using Item - based, User - based and Matrix Factorization to show personalized movie\\\\nrecommendations based on ratings given by a user and other users similar to them in order to improve user experience with\\\\nRMSE score of close to 0.8\\\\nEDUCATION\\\\nWoolf 2024\\\\nMS in Computer Science90%\\\\nKerala University 2015\\\\nBE/B.Tech/BS in Mechanical Engineering\",\"top_k\":3}', name='jd_search')], type='ToolCallRequestEvent'),\n",
       " ToolCallExecutionEvent(id='4adc6dc3-384b-4e61-a7d3-7805f8e04708', source='job_description_analysis_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 12, 18, 7, 6, 292235, tzinfo=datetime.timezone.utc), content=[FunctionExecutionResult(content='Error occurred while searching: Collection [5ff7004d-2c5b-4388-ba16-a4d1e61d623f] does not exists.', name='jd_search', call_id='call_PUjgeXvl3E38XuFdApFMMVZJ', is_error=False)], type='ToolCallExecutionEvent'),\n",
       " ToolCallSummaryMessage(id='b44ccaa0-5e6f-4b7a-b3de-5dd6c1ac9fe2', source='job_description_analysis_agent', models_usage=None, metadata={}, created_at=datetime.datetime(2025, 8, 12, 18, 7, 6, 292303, tzinfo=datetime.timezone.utc), content='Error occurred while searching: Collection [5ff7004d-2c5b-4388-ba16-a4d1e61d623f] does not exists.', type='ToolCallSummaryMessage', tool_calls=[FunctionCall(id='call_PUjgeXvl3E38XuFdApFMMVZJ', arguments='{\"resume_data\":\"Adarsh ReghuvaranMachine Learning Engineer\\\\n+91-9400831199 Linkedin adarsh.reghuvaran@gmail.com\\\\nSKILLS\\\\n•Machine Learning, Deep Learning, Python, SQL, Pytorch, Power BI, Agentic AI, Tableau, Tensorflow, Generative AI,\\\\nPrompt Engineering, Google Cloud Platform, AWS, Scikit-learn, Pandas, Seaborn, Matplotlib, Numpy, Data wrangling, Python, Azure,\\\\nData Analysis\\\\nEXPERIENCE\\\\nData Scientist Jan 2025 - Present\\\\nSaraca Solutions\\\\nDesigned and implemented a multi-agent conversational AI system using LangGraph to coordinate specialized agents for complex\\\\nquestion answering. The system uses RAG to pull contextually relevant information from multiple document sources.\\\\nDesigned and developed an AI-powered chatbot using LangChain and RAG (Retrieval-Augmented Generation) to handle\\\\nemployee queries efficiently.\\\\nMachine Learning Engineer Dec 2021 - Jan 2025\\\\nTrenser Technology Solutions (P) Ltd.\\\\nBuilt medical companion chat-bot which gives medical advices to user based on user\\'s medical history as well as information from\\\\nmedical documents using Retrieval-Augmented Generation (RAG). Chat-bot was build using openai\\'s GPT-4.\\\\nWorked in the development of Surgical assistance tool which inclues implementing research paper techniques to improve the\\\\nexisting deep learning model based on FPN architecture such as implemented ST++(https://arxiv.org/abs/2106.05095), making neck\\\\nof the model lighter to reduce inference time, reduced the time in image processing module from 300ms to 3ms by trying out\\\\ndifferent image processing techniques using OpenCV and Numpy\\\\nImproved existing object detection model used in mobile application by trying out data augmentation, increased the size of minority\\\\nclass images using data augmentatation and changing entire model which had improved to accuracy on test set from 94% to 97%.\\\\nAssistant Data Scientist Sep 2020 - Dec 2021\\\\nCurvelogics Advanced Technology Solutions Pvt Ltd\\\\nDesigned and implemented NLP-based system to classify documents using Machine learning Naive Bayes classifier and extract\\\\ninformation from documents using NLTK, Scipy and Regular expression\\\\nBuilt tool for analyzing people’s sentiment towards a topic by analyzing recent tweets using Large Language models such as\\\\nBERT, T5 etc using huggingface library and Tensorflow. Responsiblities include collecting training data, training model and\\\\nevaluation.\\\\nBuild Segmentation model to identify dents/cracks in vehicles for an insurance company using Detectron model. Responsiblities\\\\ninclude labeling training data and model training.\\\\nBuild tracking mechanism in CCTV live feeds to track people and send notification when detected.\\\\nBuild Classification model using Light Gradient Boost Classifier to predict if the client will subscribe an isurance using direct\\\\nmarketing campaign for a banking institution using heavily unbalanced dataset which contains only 8% positive class. Model was\\\\nable to achieve recall of 0.77 on test set.Responsiblities include feature engineering, outlier removal and model building.\\\\nPROJECTS\\\\nNLP: Named Entity Recognition\\\\nhttps://github.com/adarsh666/NER/blob/main/Twitter_NER.ipynb\\\\nUtilized Named Entity Recognition (NER) to automatically identify and extract important content from tweets, contributing to\\\\nenhanced understanding of user-generated content and trends.\\\\nModel used was bert-base-uncased using huggingface library. Model was able to achieve accuracy of 97.11% in traing set and\\\\n96.21% on test set.\\\\nRecommender System\\\\nhttps://github.com/adarsh666/RecommenderSystem/blob/main/Recommender_Systems.ipynb\\\\nCreated Recommender Systems using Item - based, User - based and Matrix Factorization to show personalized movie\\\\nrecommendations based on ratings given by a user and other users similar to them in order to improve user experience with\\\\nRMSE score of close to 0.8\\\\nEDUCATION\\\\nWoolf 2024\\\\nMS in Computer Science90%\\\\nKerala University 2015\\\\nBE/B.Tech/BS in Mechanical Engineering\",\"top_k\":3}', name='jd_search')], results=[FunctionExecutionResult(content='Error occurred while searching: Collection [5ff7004d-2c5b-4388-ba16-a4d1e61d623f] does not exists.', name='jd_search', call_id='call_PUjgeXvl3E38XuFdApFMMVZJ', is_error=False)])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3925019c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error occurred while searching: Collection [5ff7004d-2c5b-4388-ba16-a4d1e61d623f] does not exists.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.messages[-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5aad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
